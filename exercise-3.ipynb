{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "! pip install --quiet gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "yelps = pd.read_csv('sentiment/yelps.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def clean_yelp(s):\n",
    "    s = BeautifulSoup(s, 'html5lib').get_text()\n",
    "    s = ' '.join(s.split())\n",
    "    s = s.strip().lower()\n",
    "    return s\n",
    "\n",
    "def tokenize(s):\n",
    "    \"\"\" Returns a list of strings (tokens) from the document \"\"\"\n",
    "\n",
    "    # Implement this function!\n",
    "    # Hint: If you look at the sklearn source code on github, \n",
    "    # you will find a good regular expression they use as a default\n",
    "    # tokenizer in CountVectorizer -- this is the tokenizer\n",
    "    # we have been using until now!\n",
    "    # But you need to implement your own now. \n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "# Create a count vectorizer, using the preprocessor and tokenizer from above\n",
    "# use TruncatedSVD from Sklearn to generate a term embedding matrix for your data\n",
    "# that is 100 dimensional in size.\n",
    "# Embed your documents via that term embedding matrix (this is just the PCA of the \n",
    "# TF matrix, as returned by \"transform\" on TruncatedSVD\n",
    "\n",
    "# Here you should have TWO vector representations of your Yelp data: \n",
    "# 1. The original TF matrix\n",
    "# 2. The first 100 components of the PCA of that matrix, as performed by TruncatedSVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def test_holdout_vectors(V, y, cutoff):\n",
    "    model = LogisticRegression()\n",
    "    model.fit(V[:cutoff], y[:cutoff])\n",
    "    preds = model.predict(V[cutoff:])\n",
    "    return accuracy_score(preds, y[cutoff:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# Test your vector representations using the above function, test_holdout_vectors. \n",
    "# for several levels of \"cutoff\". \n",
    "\n",
    "# This function simulates training on a small part of your data and testing on the rest.\n",
    "# Thus, it's as though you have some labelled data but a bunch of unlabelled data. \n",
    "# This is essentially a semi-supervised situation. \n",
    "\n",
    "# Do the word embeddings generated via SVD allow you to pull information from the \n",
    "# unlabeled data and improve your score given a small training set? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "# Next, create another term embedding via Word2Vec, also 100 dimensional\n",
    "# Look at the documentation from the library gensim and see what hyperparameters\n",
    "# you can choose to tune. \n",
    "# Note: training takes time, so you probably won't want to try too many!\n",
    "\n",
    "# pass the cleaned and tokenized yelp reviews as \"sentences\" to your model\n",
    "# to build the vocabulary and train the word vectors. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# use the embed_w2v function to embed your Yelp reviews using the new\n",
    "# word2vec word embeddings. Each review will be a normalized sum of the\n",
    "# words that it is made of. \n",
    "# See how this improves the scores in the classification task using\n",
    "# test_holdout_vectors -- is word2vec better? At what training-set sizes does it help\n",
    "# more? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def embed_w2v(tokens, w2v):\n",
    "    idxs = [w2v.wv.vocab.get(t) for t in tokens]\n",
    "    idxs = [t.index for t in idxs if t]\n",
    "    N = w2v.wv.vectors.shape[1]\n",
    "    if len(idxs) < 1:\n",
    "        return np.zeros(N)\n",
    "    a = np.sum(w2v.wv.vectors[idxs, :], axis=0)\n",
    "    a /= np.linalg.norm(a)\n",
    "    return a\n",
    "\n",
    "\n",
    "# use as follows: np.array([embed_w2v(t, w2v) for t in X])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from functools import reduce\n",
    "\n",
    "pats = [re.compile(r'https?:\\/\\/[^\\s]+'), \n",
    "        re.compile(r'rt\\s*@\\w+\\s?:?'),\n",
    "        re.compile(r'@\\w+')]\n",
    "\n",
    "def clean_twitter(s):\n",
    "    \"\"\" Cleans Twitter specific issues\n",
    "    \n",
    "    Should probably clean out mentions, URLs, and RT's.\n",
    "    \"\"\"\n",
    "    s = s.lower()\n",
    "    _sub = lambda s,pat: re.sub(pat, '', s)\n",
    "    s = reduce(_sub, pats, s)\n",
    "    s = ' '.join(s.split()).strip()\n",
    "    # TODO: Use regular expressions to remove unwanted\n",
    "    # text and clean up our tweets to be more usable!\n",
    "\n",
    "    # BONUS: Try using the library \"spacy\" to \n",
    "    # do further processing, such as lemmatizing\n",
    "    # or replacing Named Entities with constants (i.e. \"[NAMED]\")\n",
    "    # or adding the part of speech or dependency code to the word \n",
    "\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# Load tweet!\n",
    "# NOTE: this file needs to be downloaded from Box! \n",
    "\n",
    "with open('tweets/tweets.txt') as f:\n",
    "    tweets = pd.Series(f.readlines())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# Now embed this twitter data, again using Word2Vec and LSA (SVD)\n",
    "# you don't have any labels, but you might be able to \n",
    "# find some interesting phenomena. \n",
    "\n",
    "# Explore using either or both of the following tools: \n",
    "# 1. Word2Vec has a built in feature for getting most similar\n",
    "# words, including via positive/negative examples (subtraction)\n",
    "# What happens when you subtract \"hillary\" from \"trump\"? \n",
    "# \n",
    "# Bonus: build a similar feature for your LSA features and\n",
    "# see if you can do similar vector math\n",
    "\n",
    "# 2. Use the visualize_dist function to see how\n",
    "# the tweet-tweet distance looks for a selection\n",
    "# of random tweets. Do you agree with the distance? \n",
    "# Does one embedding seem better than another? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import pdist,squareform\n",
    "import seaborn as sns\n",
    "\n",
    "def visualize_dist(V, tweets, idxs):\n",
    "    \"\"\" Visualize distances in heatmap\n",
    "\n",
    "    V is the vector of embeddings \n",
    "    tweets is a list or series that converts from idx to tweet\n",
    "    idxs is a list of integers that you would like to compare (<= 5 probably)\n",
    "    \"\"\"\n",
    "    m = squareform(pdist(V[idxs], 'cosine'))\n",
    "    _ = sns.heatmap(pd.DataFrame(m, columns = tweets[idxs], index= tweets[idxs]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# More bonus!\n",
    "# Use one of the following pre-embedded models and see if your yelp scores improve? \n",
    "# What about your Twitter explorations?\n",
    "\n",
    "# https://github.com/RaRe-Technologies/gensim-data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "name": "exercise-3.ipynb"
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
