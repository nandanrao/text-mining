{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "autoscroll": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['figure.figsize'] = [14, 10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "autoscroll": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.manifold import TSNE, MDS\n",
    "from bs4 import BeautifulSoup\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "\n",
    "def preprocess(s):\n",
    "    s = BeautifulSoup(s, 'html5lib').get_text()\n",
    "    s = ' '.join(s.split())\n",
    "    s = s.strip().lower()\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "autoscroll": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# load and randomize\n",
    "\n",
    "yelps = pd.read_csv('sentiment/yelps.csv').sample(frac=1.)\n",
    "movies = pd.read_csv('sentiment/movies.csv').sample(frac=1.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## Inspecting p(X) of the BOW vectorizers\n",
    "\n",
    "Assuming that our data consists of discrete features, as in the Bag of Words models we have seen so far, visualizing $p(X)$ consists of visualizing the PMF of a multionomial distribution that consists of all the features in our data. \n",
    "\n",
    "Of course, this is a very wide distribution, even if we just include unigrams. \n",
    "\n",
    "Thus, we will visualize just the most popular features. \n",
    "\n",
    "We will see how, given the most popular features of one dataset, the other dataset compares. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "autoscroll": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "\n",
    "def _train_and_transform(vectorizer, X):\n",
    "    vectorizer.fit(X)\n",
    "    return vectorizer, vectorizer.transform(X)\n",
    "\n",
    "def compare(vectorizer, A, B):\n",
    "    vec_A, VA = _train_and_transform(deepcopy(vectorizer), A) \n",
    "    vec_B, VB = _train_and_transform(deepcopy(vectorizer), B) \n",
    "\n",
    "    # Get the most popular words for A\n",
    "    # ignoring the top 25 super common\n",
    "    # words\n",
    "    sums = np.array(VA.sum(0)).reshape(-1)\n",
    "    tops = np.argsort(sums)[-325:-25]\n",
    "    freqs_A = sums[tops]\n",
    "\n",
    "    features = np.array(vec_A.get_feature_names())[tops]\n",
    "\n",
    "    idxs = np.array([vec_B.vocabulary_.get(s) for s in features])\n",
    "    sums = np.array(VB.sum(0)).reshape(-1)\n",
    "    freqs_B = [sums[i] if i else 0. for i in idxs]\n",
    "\n",
    "    return pd.concat([pd.DataFrame({ 'word': features, 'dataset': d, 'freq': f }) \n",
    "                      for d,f in [('A',freqs_A), ('B',freqs_B)]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "autoscroll": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(preprocessor = preprocess, min_df = 1, use_idf=False)\n",
    "df = compare(vectorizer, yelps.sample(500).text, movies.sample(500).text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "sns.barplot(x = 'word', \n",
    "            y = 'freq', \n",
    "            data = df[df.dataset == 'A'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "sns.barplot(x = 'word', \n",
    "            y = 'freq', \n",
    "            data = df[df.dataset == 'B'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## Generative vs. Discriminative\n",
    "\n",
    "Play around with the following plotting function. Is this consisten with the theoretical results of Ng and Jordan? Can you get the opposite results? Why? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# add vectors here\n",
    "V = \"foo\"\n",
    "y = yelps.positive\n",
    "\n",
    "def _cv_score(V, y, models):\n",
    "    return [{ 'name': name, 'value': np.mean(cross_val_score(model, V, y, cv=3)) }\n",
    "            for name, model in models]\n",
    "\n",
    "def plot_cv_scores(V, y, models, ticks):\n",
    "    scores = [_cv_score(V[:t], y[:t], models) for t in ticks]        \n",
    "    scores = [({'N': np.log(t) , **i}) for t,s \n",
    "              in zip(ticks, scores) for i in s]\n",
    "    df = pd.DataFrame(scores)\n",
    "    return sns.lineplot(y='value', x='N', hue='name', data=df)\n",
    "\n",
    "models = [('NB', MultinomialNB(fit_prior=False)), \n",
    "          ('LR', LogisticRegression())]\n",
    "\n",
    "plot_cv_scores(V, y, models, [300, 600, 1200, 2400, 4800, 9600, 19200])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## Searching the Grid\n",
    "\n",
    "Hyperparameter tuning, in its simplest form, can be done via a grid search. \n",
    "\n",
    "We can use cross validation to estimate the out-of-sample expected risk. \n",
    "\n",
    "SKlearn gives us a very nice package for this: GridSearchCV\n",
    "\n",
    "The API can be a bit complicated at first glance, but it's simple once you get used to it and comfortable with the idea of a \"pipeline\". "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## Crosstraining\n",
    "\n",
    "Test how well your model, when trained on one dataset, performs on the other. \n",
    "\n",
    "To do this, make sure that you: \n",
    "\n",
    "1. You perform the \"fitting\" of both the vectorizer and the model on one of the sets\n",
    "2. Use the \"transform\" of the vectorizer to transform the second dataset into the same feature space (X) that your model was trained on. \n",
    "3. Use the \"predict\" of your model to see how well it did on the second dataset. \n",
    "\n",
    "How well does your model generalize from one to the other? What does this say about \"sentiment\"? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# Perform this on a smaller sample of the data to not break your machine\n",
    "\n",
    "yelps = yelps.sample(10000)\n",
    "movies = movies.sample(10000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "name": "exercise-2.ipynb",
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
